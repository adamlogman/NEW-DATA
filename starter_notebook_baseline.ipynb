{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dry Spell Warning â€” Starter Notebook (Baseline)\n",
        "\n",
        "This notebook shows how to:\n",
        "\n",
        "1. Load a few provided feature files\n",
        "2. Load the training labels (`solution_train.csv`)\n",
        "3. Train a simple Logistic Regression baseline\n",
        "4. Generate `submission.csv` and `submittion.zip`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "DATA_DIR = Path(\"../public_data/input_data\")\n",
        "TRAIN_DIR = DATA_DIR / \"train\"\n",
        "TEST_DIR  = DATA_DIR / \"test\"\n",
        "\n",
        "LABEL_COL = \"dryspell_warn_7d\"   # target label column in solution_train.csv\n",
        "DATE_COL  = \"date\"              # merge key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load feature files + labels (train)\n",
        "\n",
        "We will load:\n",
        "- `sea level pressure era5_train.csv`\n",
        "- `temp_trimmed_train.csv`\n",
        "- `vapour pressure deficit_train.csv`\n",
        "- `solution_train.csv`\n",
        "\n",
        "Then merge them on `date`.\n",
        "\n",
        "After Downloading the data from the link in the `data` tab, you can run The following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
        "    return df\n",
        "\n",
        "slp = load_csv(TRAIN_DIR / \"sea level pressure era5_train.csv\")\n",
        "tmp = load_csv(TRAIN_DIR / \"temp_trimmed_train.csv\")\n",
        "vpd = load_csv(TRAIN_DIR / \"vapour pressure deficit_train.csv\")\n",
        "y   = load_csv(TRAIN_DIR / \"solution_train.csv\")\n",
        "\n",
        "print(\"SLP columns:\", slp.columns.tolist())\n",
        "print(\"TEMP columns:\", tmp.columns.tolist())\n",
        "print(\"VPD columns:\", vpd.columns.tolist())\n",
        "print(\"Y columns:\", y.columns.tolist())\n",
        "\n",
        "df_train = y.merge(slp, on=DATE_COL, how=\"inner\") \\\n",
        "            .merge(tmp, on=DATE_COL, how=\"inner\") \\\n",
        "            .merge(vpd, on=DATE_COL, how=\"inner\")\n",
        "\n",
        "df_train = df_train.sort_values(DATE_COL).reset_index(drop=True)\n",
        "df_train.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Build X/y\n",
        "\n",
        "We drop the `date` column and keep the rest as numeric features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = df_train[LABEL_COL].astype(int).values\n",
        "\n",
        "drop_cols = {DATE_COL, LABEL_COL}\n",
        "feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
        "\n",
        "X_train = df_train[feature_cols].copy()\n",
        "\n",
        "print(\"Train rows:\", len(X_train))\n",
        "print(\"Num features:\", len(feature_cols))\n",
        "print(\"Positive rate:\", float(y_train.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Train a simple Logistic Regression baseline\n",
        "\n",
        "- Impute missing values with median\n",
        "- Standardize features\n",
        "- Fit Logistic Regression\n",
        "\n",
        "This is intentionally simple and meant only as a baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=2000,\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Done training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load test features and generate predictions\n",
        "\n",
        "We load the matching test feature files, merge on `date`, and predict.\n",
        "\n",
        "We output `dryspell_warn_7d` as **0/1** by thresholding probabilities at 0.5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "slp_t = load_csv(TEST_DIR / \"sea level pressure era5_test.csv\")\n",
        "tmp_t = load_csv(TEST_DIR / \"temp_trimmed_test.csv\")\n",
        "vpd_t = load_csv(TEST_DIR / \"vapour pressure deficit_test.csv\")\n",
        "\n",
        "dfs = [slp_t, tmp_t, vpd_t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2868b37c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "dfs = [d.groupby(DATE_COL, as_index=False).mean(numeric_only=True) for d in dfs]\n",
        "\n",
        "# Union of all dates across files\n",
        "df_test = reduce(lambda left, right: left.merge(right, on=DATE_COL, how=\"outer\"), dfs)\n",
        "\n",
        "df_test = df_test.fillna(0)\n",
        "\n",
        "# Sort + clean\n",
        "df_test[DATE_COL] = pd.to_datetime(df_test[DATE_COL])\n",
        "df_test = df_test.sort_values(DATE_COL).reset_index(drop=True)\n",
        "\n",
        "X_test = df_test[feature_cols].copy()\n",
        "\n",
        "proba = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "submission = pd.DataFrame({DATE_COL: df_test[DATE_COL], LABEL_COL: pred})\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Save `submission.csv` and `submission.zip`\n",
        "\n",
        "The zip file is what you upload to Codabench (result-upload submission).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_path = Path(\"submission.csv\")\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec25d3bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "out_path = Path(\"submission.csv\")\n",
        "zip_path = Path(\"submission.zip\")\n",
        "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    # Put submission.csv at the ZIP root (no folders)\n",
        "    zf.write(out_path, arcname=\"submission.csv\")\n",
        "\n",
        "print(\"Zipped:\", zip_path.resolve())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
